\chapter{Extended Mask R-CNN}
\label{appendix:extended-mask-rcnn}

	As stated in section \ref{sec:ai-model} and noticed in \ref{sec:discussion}, the configuration of the Mask R-CNN was not optimal. To confirm that another \ac{dnn} was trained. This appendix shows the performance of this network on the in-distribution test dataset and the sim-to-real test dataset.\\
	The configuration changes consist of three major changes. 
	\begin{itemize}
		\item \textbf{\acl{nms}:} Since goal environments are cluttered scenes, overlaying objects are common. The \ac{nms} have to be less sensitive to find objects lying on each other. 
		\item \textbf{\acl{fpn}-Layers:} An extensive adjustment of the \ac{fpn} was made to add two more layers for feature extraction of smaller objects.
		\item \textbf{Number of Epochs:} The number of epochs was increased from 50 to 100. The loss of the training with 50 epochs tends to increase further; thus, the number of epochs increases.
	\end{itemize}
	The highest amount of information was chosen for the depth and number of shapes and textures. This should theoretically lead to the most precise network.
	\begin{itemize}
		\item \textbf{Number of unique Textures:} 160
		\item \textbf{Number of unique Shapes:} 160
		\item \textbf{Using Depth as 4th Channel:} Yes
	\end{itemize}
	This appendix can only confirm that all of these changes together lead to better performance and does not analyze the isolated influences.\\
	Due to time limitations, this improved configuration could not be applied to the models in this study.
	
	% show results
	\section{Results}
	
	
	
	% conclusion -> better
	\section{Conclusion}
	
	
	
	% fpn adjstment
	\section{\ac{fpn} Adjustment}
		To add more layers to the \acl{fpn}, there are significant changes to make. First, the output of the ResNet has to change, as well as the \ac{fpn} initialization and forwarding, and the backbone's whole process.\\
		Then the \ac{rpn}-Anchors must be adjusted since there are more feature maps to create proposals from.\\
		Then, the box and mask heads must be adjusted for the increased number of input layers.
	
	\begin{lstlisting}[language=Python,caption=Random Augmentation of the Background using cv2 in Python, label=lst:bg-augmentation]
import torch
import torch.nn as nn
import torch.nn.functional as F

import torchvision
from torchvision.ops import MultiScaleRoIAlign
from torchvision.models.detection import MaskRCNN
from torchvision.models.detection.mask_rcnn import MaskRCNNHeads, MaskRCNNPredictor
from torchvision.models.detection.rpn import RegionProposalNetwork, RPNHead
from torchvision.models.detection.roi_heads import RoIHeads
from torchvision.models.detection.faster_rcnn import TwoMLPHead, FastRCNNPredictor
from torchvision.models.detection.anchor_utils import AnchorGenerator
from torchvision.models.detection.transform import GeneralizedRCNNTransform
from torchvision.models import ResNet50_Weights
from torchvision.models.resnet import resnet50


class Extended_ResNet(nn.Module):
		def __init__(self):
				super().__init__()
				self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
				# add first layers
				self.c1 = nn.Sequential(self.backbone.conv1, self.backbone.bn1, self.backbone.relu) 
				self.c2 = nn.Sequential(self.backbone.maxpool, self.backbone.layer1) 
				self.c3 = self.backbone.layer2  
				self.c4 = self.backbone.layer3  
				self.c5 = self.backbone.layer4  
				
				# just for Mask R-CNN
				self.conv1 = self.backbone.conv1
				self.bn1 = self.backbone.bn1
				self.layer2 = self.backbone.layer2
				self.layer3 = self.backbone.layer3
				self.layer4 = self.backbone.layer4
		
		def forward(self, x):
				c1 = self.c1(x)
				c2 = self.c2(c1)
				c3 = self.c3(c2)
				c4 = self.c4(c3)
				c5 = self.c5(c4)
				return c1, c2, c3, c4, c5
		
		def update_conv1(self, new_conv1):
				self.conv1 = new_conv1  # Replace the old Conv1 Layer with the new one
				self.c1 = nn.Sequential(new_conv1, self.backbone.bn1, self.backbone.relu)




class Extended_FPN(nn.Module):
		def __init__(self):
				super().__init__()
				
				# Inner Blocks: 1x1 Convolutions for Channel-Resizing / Adjustment
				self.inner_blocks = nn.ModuleList([
						nn.Conv2d(64, 256, kernel_size=1),   # for C1 (64 Channels)
						nn.Conv2d(256, 256, kernel_size=1),  # for C2
						nn.Conv2d(512, 256, kernel_size=1),  # for C3
						nn.Conv2d(1024, 256, kernel_size=1), # for C4
						nn.Conv2d(2048, 256, kernel_size=1)  # for C5
				])
				
				# Layer Blocks: 3x3 Convolutions for more refined results
				self.layer_blocks = nn.ModuleList([
						nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # for P1
						nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # for P2
						nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # for P3
						nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # for P4
						nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)   # for P5
				])

		def forward(self, c1, c2, c3, c4, c5):
				
				# build feature pyramid
				p5 = self.inner_blocks[4](c5)
				p4 = self.inner_blocks[3](c4) + F.interpolate(p5, scale_factor=2, mode="nearest")
				p3 = self.inner_blocks[2](c3) + F.interpolate(p4, scale_factor=2, mode="nearest")
				p2 = self.inner_blocks[1](c2) + F.interpolate(p3, scale_factor=2, mode="nearest")
				p1 = self.inner_blocks[0](c1) + F.interpolate(p2, scale_factor=2, mode="nearest")
				
				# Apply layer blocks
				p5 = self.layer_blocks[4](p5)
				p4 = self.layer_blocks[3](p4)
				p3 = self.layer_blocks[2](p3)
				p2 = self.layer_blocks[1](p2)
				p1 = self.layer_blocks[0](p1)
				
				return [p1, p2, p3, p4, p5]



class Extended_Backbone(nn.Module):
		def __init__(self):
				super().__init__()
				self.body = Extended_ResNet()
				self.fpn = Extended_FPN()
				self.out_channels = 256
		
		def forward(self, x):
				c1, c2, c3, c4, c5 = self.body(x)  
				fpn_features = self.fpn(c1, c2, c3, c4, c5)
				return {"p1": fpn_features[0], "p2": fpn_features[1], "p3": fpn_features[2], "p4": fpn_features[3], "p5": fpn_features[4]}



def load_maskrcnn(weights_path=None, 
						use_4_channels=False, pretrained=True,
						image_mean=[0.485, 0.456, 0.406, 0.5], 
						image_std=[0.229, 0.224, 0.225, 0.5],    # from ImageNet
						min_size=1080, 
						max_size=1920, 
						log_path=None, 
						should_log=False, 
						should_print=True,
						extended_version=False):
		"""
		Load a Mask R-CNN model with a specified backbone and optional modifications.
		
		This function initializes a Mask R-CNN model with a ResNet50-FPN backbone. 
		It allows for the configuration of input channels and loading of pretrained weights.
		
		Parameters:
		-----------
		weights_path (str, optional): 
				Path to the weights file to load the model's state dict. 
				If None, the model will be initialized with random weights or 
				pretrained weights if 'pretrained' is True.
		
		use_4_channels (bool, optional): 
				If True, modifies the first convolutional layer to accept 
				4 input channels instead of the default 3. The weights from 
				the existing channels are copied accordingly.
		
		pretrained (bool, optional): 
				If True, loads the pretrained weights for the backbone. 
				Defaults to True.
		
		Returns:
		--------
		model (MaskRCNN): 
				The initialized Mask R-CNN model instance, ready for training or inference.
		"""
		if extended_version:
				backbone = Extended_Backbone()
				model = MaskRCNN(backbone, num_classes=2)  # 2 Classes (Background + 1 Object)
				
				if use_4_channels:
						# Change the first Conv2d-Layer for 4 Channels
						in_features = model.backbone.body.conv1.in_channels
						out_features = model.backbone.body.conv1.out_channels
						kernel_size = model.backbone.body.conv1.kernel_size
						stride = model.backbone.body.conv1.stride
						padding = model.backbone.body.conv1.padding
						
						# Create new conv layer with 4 channels
						new_conv1 = torch.nn.Conv2d(4, out_features, kernel_size=kernel_size, stride=stride, padding=padding)
						
						# copy the existing weights from the first 3 Channels
						with torch.no_grad():
						new_conv1.weight[:, :3, :, :] = model.backbone.body.conv1.weight  # Copy old 3 Channels
						new_conv1.weight[:, 3:, :, :] = model.backbone.body.conv1.weight[:, :1, :, :]  # Init new 4.th Channel with the one old channel
						
						# update model
						model.backbone.body.update_conv1(new_conv1)
						
						# Modify the transform to handle 4 channels
						model.transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)
				
				out_channels = model.backbone.out_channels
				
				# update RPN
				rpn_anchor_generator = AnchorGenerator(
						sizes=(
								(16, 32, 64),        # Sizes for level 0
								(32, 64, 128),       # Sizes for level 1
								(64, 128, 256),     # Sizes for level 2
								(128, 256, 512),   # Sizes for level 3
								(256, 512, 1024),  # Sizes for level 4
						),
						aspect_ratios=(
								(0.25, 0.5, 1.0),  # Aspect ratios for level 0
								(0.25, 0.5, 1.0),  # Aspect ratios for level 1
								(0.25, 0.5, 1.0),  # Aspect ratios for level 2
								(0.25, 0.5, 1.0),  # Aspect ratios for level 3
								(0.25, 0.5, 1.0),  # Aspect ratios for level 4
						),
				)
				
				rpn_head = RPNHead(
						out_channels, 
						rpn_anchor_generator.num_anchors_per_location()[0]
				)
				rpn_fg_iou_thresh = 0.7
				rpn_bg_iou_thresh = 0.3
				rpn_batch_size_per_image = 256
				rpn_positive_fraction = 0.5
				rpn_pre_nms_top_n = dict(training=2000, testing=1000)
				rpn_post_nms_top_n = dict(training=2000, testing=1000)
				rpn_nms_thresh = 0.7
				score_thresh = 0.0
				
				rpn = RegionProposalNetwork(
					rpn_anchor_generator,
					rpn_head,
					rpn_fg_iou_thresh,
					rpn_bg_iou_thresh,
					rpn_batch_size_per_image,
					rpn_positive_fraction,
					rpn_pre_nms_top_n,
					rpn_post_nms_top_n,
					rpn_nms_thresh,
					score_thresh=score_thresh,
				)
				model.rpn = rpn
				
				# Update RoI, Box and Mask Head
				box_roi_pool = MultiScaleRoIAlign(
						featmap_names=["p1", "p2", "p3", "p4", "p5"], 
						output_size=7, 
						sampling_ratio=2
				)
				
				resolution = box_roi_pool.output_size[0]
				representation_size = 1024
				box_head = TwoMLPHead(out_channels * resolution**2, representation_size)
				
				representation_size = 1024
				box_predictor = FastRCNNPredictor(representation_size, 2)
				
				box_fg_iou_thresh = 0.5
				box_bg_iou_thresh = 0.5
				box_batch_size_per_image = 512
				box_positive_fraction = 0.25
				bbox_reg_weights = None
				box_score_thresh = 0.05
				box_nms_thresh = 0.5
				box_detections_per_img = 100
				
				mask_roi_pool = MultiScaleRoIAlign(
						featmap_names=["p1", "p2", "p3", "p4", "p5"], 
						output_size=14, 
						sampling_ratio=2
				)
				mask_layers = (256, 256, 256, 256, 256)
				mask_dilation = 1
				mask_head = MaskRCNNHeads(out_channels, mask_layers, mask_dilation)
				
				mask_predictor_in_channels = 256  
				mask_dim_reduced = 256
				mask_predictor = MaskRCNNPredictor(mask_predictor_in_channels, mask_dim_reduced, 2)
				
				roi_heads = RoIHeads(
						# Box
						box_roi_pool,
						box_head,
						box_predictor,
						box_fg_iou_thresh,
						box_bg_iou_thresh,
						box_batch_size_per_image,
						box_positive_fraction,
						bbox_reg_weights,
						box_score_thresh,
						box_nms_thresh,
						box_detections_per_img
				)
				roi_heads.mask_roi_pool = mask_roi_pool
				roi_heads.mask_head = mask_head
				roi_heads.mask_predictor = mask_predictor
				
				model.roi_heads = roi_heads
				
				# other parameter adjustments
				# adjust loss weights
				model.rpn.rpn_cls_loss_weight = 1.0
				model.rpn.rpn_bbox_loss_weight = 2.0
				model.roi_heads.mask_loss_weight = 2.0
				model.roi_heads.box_loss_weight = 1.0
				model.roi_heads.classification_loss_weight = 1.0
				
				# adjust non-maximum suppression
				model.roi_heads.nms_thresh = 0.2
				model.roi_heads.box_predictor.nms_thresh = 0.2 
				model.roi_heads.mask_predictor.mask_nms_thresh = 0.2 
				model.roi_heads.score_thresh = 0.4  
		else:
			[...]   # normal way
		
		[...]    # loading weights + printing architecture
		
		return model
	\end{lstlisting}
	
	


