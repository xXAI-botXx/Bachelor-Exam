\chapter{Conclusion}
\label{chap:kapitel6}
	This work proposes nine labeled RGBD datasets, each with 20,000 images, for training with varying amounts of shapes and textures; four labeled RGBD datasets for testing with varying novelness of shapes and textures, each with 100 images; one small set of 8 exceptional scenarios with puzzling textures; and one real-world RGBD dataset for bin-picking with 70 images.\\
	Furthermore, this work presents findings regarding depth information and the amount of shapes and textures in instance segmentation, as described in section \ref{sec:summary-of-findings}.


	\section{Summary of Findings}
	\label{sec:summary-of-findings}
		This study investigated the influence of depth information and the amount of shape and texture towards shape-texture bias and performance on instance segmentation using models trained with the 3xM datasets. Several key findings emerged:
		\begin{enumerate}
			\item \textbf{Shape vs. Texture Bias:} RGB-only models displayed a clear bias toward texture, while RGB-D models shifted slightly toward shape bias but still exhibited texture bias. Interestingly, increasing the number of shapes often decreased shape bias, challenging the hypothesis that more diverse shapes lead to stronger shape bias.
			
			\item \textbf{In-Distribution Performance:} Models trained with both RGB and depth data achieved a slight improvement (1.5\% mean IoU) over RGB-only models. However, these improvements were context-dependent, with significant variations across different experimental setups. The experiments show that additional depth information can improve the accuracy of Mask R-CNN by unknown textures.\\
			Increasing the number of shapes and textures generally improved performance. The performance especially increases with a higher number of shapes with unknown shapes; also with a higher amount of unknown textures, increasing the texture amount in the training data improves the accuracy of the \ac{dnn}'s mask prediction. 
			\\
			\\
			\item \textbf{Sim-to-Real Performance:} Sim-to-real experiments revealed divergent results. RGB-D models excelled on the OCID dataset, achieving 21.34\% higher mean IoU than RGB-only models, but performed worse on the Optonic Bin-Picking dataset. This suggests that dataset-specific factors such as sharpness, texture quality, and lighting conditions heavily influence performance. Notably, RGB-only models outperformed RGB-D models by 22.26\% on the Optonic Bin-Picking dataset, likely due to differences in data quality and sharpness.\\
			The rising accuracy using increasing shape amounts could depend on the number of unknown shapes. There also seems to be a relation between the rising accuracy in sim-to-real from increasing the number of textures and the number of unknown textures in the real-world dataset.
			
			\item \textbf{Generalization:} The hypothesis that depth data improves generalization was not fully supported. While depth data mitigated the lack of novel textures, it did not enhance generalization toward novel shapes. The quality of both RGB and depth data appears to play a crucial role, highlighting the importance of image sharpness and resolution.\\
			The number of shapes appears to influence the network's ability to generalize to new shapes, particularly when encountering unfamiliar ones. Similarly, the number of textures tends to enhance the generalization of textures, especially when novel textures are present in the data.
		\end{enumerate}
		
		These findings demonstrate the complex interplay of depth information and shape-texture amount in influencing the performance and generalization of \ac{dnn}s in instance segmentation tasks. They underscore the importance of dataset diversity and quality in shaping model biases and achieving robust performance and generalization across varied contexts.
	
	
	\section{Implications and Future Work}
	\label{sec:implications-and-future-work}
		Although this work acquired new empirical values and trends, there are many open questions to answer. This section describes open questions and recommended future research.
		
		\textbf{Depth Information Quality} could be a crucial factor for influencing the performance and sim-to-real ability, as stated in section \ref{sec:interpretation-of-results}. While the influence of RGB-image quality is already covered by research, the influence of depth information quality remains open and should be further investigated. The ambiguous result from the sim-to-real experiment in \ref{sec:results} hints at the importance of depth information quality but still can not prove it. Further sim-to-real experiments could help to confirm or reject this hypothesis. If this hypothesis is true, then research about augmentation to reduce the negative impact of noisy depth sensors would be very important.
		\clearpage
		The influence of the \textbf{\ac{dnn} Architecture} is not part of the experiments of this work. It could lead to intriguing results to run the here used experiments on other \ac{dnn} architectures. In detail, it would be interesting to see the results from different kinds of architectures, like a comparison between \ac{cnn} based, transformer, and reinforcement architectures. This could yield novel insights into the different architectures and their processing differences. Is the influence of depth information and shape-texture amount similar over different \ac{dnn} architectures?
		
		A fascinating research would be the investigation of the influence of different \textbf{integrations of depth information} towards in-distribution performance, sim-to-real ability, and generalization. The proposed experiments with different depth information handling \ac{dnn}s could give insights into the performance of different integrations of depth information and how the shape-texture amount can influence them. Depth information can be provided as the fourth channel (the here used approach) but also can be incorporated through specialized architectures, such as separate depth-processing branches, solely use or fusion layers, to improve the usage of depth information's unique spatial and geometric properties.
		
		Further \textbf{Sim-To-Real} experiments in other domains could assess the applicability of this study's conclusions.
		
		In addition, the Shape vs. Texture Attention Test could be extended with \textbf{explainability} methods to better understand the decision-making processes, which could lead to more precise results regarding the influence of depth information and shape-texture amount on the shape-texture bias of Mask R-CNN in instance segmentation.
		
		Moreover, a similar investigation with \textbf{higher numbers} of shapes and textures could lead to novel insights about the limits and potentials of higher amounts. This work suggests that further shapes and textures would increase the generalization ability for shapes and textures even more. The quality of this rise is uncertain.
		
		Finally, this work recommends an investigation into the roles of image sharpness (of RGB and depth data), lighting conditions, and texture quality in influencing sim-to-real results. It would also be valuable to determine more essential factors and how they influence the instance segmentation performance.
	
		% using depth in another way? => same results?
		% ...
		% Investigating other input types, like only depth images, would be a interesting comparison and could give more insights.
		% ...
		% It also cold be interesting to make a similar investigation towards robustness. Robustness is a import factor of a \ac{dnn}, especially in sensitive fields, like autonomous driving, where new factors like new light settings happens all the time and live could depend on the robustness of a \ac{dnn}.
		
	\clearpage
	\section{Challenges and Learnings}    % Learnings
	\label{sec:challenges}
		During this study, many challenges occur, and maybe the solutions of them can help in the future.\\
		The journey started with an NVIDIA GeForce GTX 1080 Ti graphic card and Mask R-CNN implementation from Matterport \cite{Matterport}. The implementation from Matterport only works with old versions of Tensorflow (Tensorflow was one given restriction), so the first task was to update the code from Matterport to a newer version. A strange, unwanted behavior occurred, where loading the same weights led to different, random results. Debugging was difficult due to the massive size of the \acl{dnn}. \\
		Switching to an already finished upgraded Mask R-CNN seemed to make more sense, but all five tried implementations did not want to work properly. So, the next approach was to try the old version of Mask R-CNN from Matterport. This old implementation came with massive problems with the newer Linux system and the old Python version needed for the Tensorflow version. The solution was virtual python environments with Anaconda \cite{anaconda}. These conda environments worked like a breeze and were also easy to replicate. This version worked, but the training had to stop since the graphic card changed to the newer NVIDIA RTX4090, and the Tensorflow version would not work on this new \ac{gpu}. It was finally time to switch to another architecture. YOLACT \cite{Bolya2019} was now attempted. First, an unofficial Tensorflow version, but quickly changed to the official PyTorch version; thus, there was already much time spent, and it seemed to be not working very well again, so the restriction of Tensorflow was lifted. This official YOLACT implementation also did not work with the new \ac{gpu} and was upgraded. During the upgrade, the code underwent tremendous changes to make it more accessible in Python; before, it was programmed to be used with parameters and not within Python. After all that work was done, the \ac{dnn} seemed to work, but it turned out that something was wrong with the network. It always found too many masks, and debugging was a large endeavor. At that time, only about one to two months were left (the data generation also needed much time to get work), so the decision was to try out the Mask R-CNN again, but with PyTorch's official implementation. PyTorch's implementation worked surprisingly well and quickly. \\
		The next challenge is the most common all over the world: time. There was little time left, but the \ac{dnn} needed much time to train. The ideal would be 100 to 500 epochs for the best results. With 18 \ac{dnn} to train and a time of 3 days with 100, the study would take 54 days only for the training.\\
		So, a second remote computer got leased from \cite{shadow}, and epochs were reduced to 20. The results were not optimal and not acceptable. So, a third remote computer was leased, and the epochs increased to 40. Three remote computers, each six \ac{dnn} to train with about 1 to 2 days computation time per network, make about two weeks to complete the whole training process. \\
		Many other challenges appeared between these challenges, like operating system failures, changing hardware, SSH remote connection issues, storage shortage, \ac{gpu} tribe difficulties,  system-specific inconsistencies, and many more.\\
		Another challenging part of this study was the creation of the synthetic datasets. The proposed data generator in Unreal Engine 5 needed much work and a finishing touch to be in a ready-to-use state. It started with collecting enough shapes and materials of a high quality and different appearance and continued with the programming of the data generator itself. It was challenging to get along with the - for games designed - Unreal Engine 5 and to render and save a virtual camera. Also, the creation of the segmentation masks took work. Moreover, many bugs occurred with the spawning objects, like rotating too fast, wrong scaling and falling out of the box. Most bugs were fixed, but the data generation is still imperfect.\\
		In the end, the generation of 9 datasets needed a long time, about ten days, and every small change needed a restart of the whole generation process.\\
		To carry it to the extreme, in the middle of this study, a personal challenge from the author popped up out of nowhere. The right side of the author's face experienced complete paralysis called peripheral facial paralysis. He visits many medical institutions for several months, from hospitals to house doctors, otologists, alternative practitioners, acupuncture, and neurologists. The right eye was at risk of drying out, and the sense of taste was strange due to the numbness. \\
		In the end, the author is grateful to have faced all these challenges with a lot of learnings and completed this work despite all its difficulties.\\
		In summary, these challenges provided many learnings. First, effective time and prioritization management are fundamental to achieving goals in time and with limited resources. With a well-thought-out time and prioritization management, time can quickly be well-spent.\\
		Secondly, overcoming unexpected obstacles and finding creative solutions with flexibility and persistence is important. The change to other architectures during this study took too long and caused much trouble at the end of this work. Also, clear and open communication is helpful and worthwhile. Miscommunication or a lack of communication can cause much extra work and cause new problems. 
		More specifically, avoiding unforeseen (but often occurring) issues with different operating systems can be tackled with virtual environments. Virtual environments can lead to mainly deterministic behavior; thus, they should be integrated into the working and experiment pipeline. Software like Anaconda or Docker can achieve such deterministic behavior. Docker was not used during this work, but it would lead to much fewer issues with different operating systems. This work used Anaconda, as described, as a virtual Python environment, which has already made this work much easier.\\
		The last learning is to \textit{fail fast}. Failing is part of everybody's life and every project. Failing can be full of learning and improvement, but it is important to fail fast in a time-limited project. During debugging, it is recommended that the code be designed and adjusted so that it fails fast if it even fails.
		
		% Learning...
		
	
	
	
	
	
	


